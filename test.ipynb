{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd notebooks/YahooEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from models import DistributedBagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the paths\n",
    "cwd = Path.cwd()\n",
    "data_path = cwd / 'data'\n",
    "# Read the data\n",
    "classes = pd.read_csv(data_path / 'classes.txt')\n",
    "train = pd.read_csv(data_path / 'train.csv', header=None)\n",
    "train.rename(columns={0: 'label', 1: 'question_title', 2: 'question_content', 3: 'best_answer'}, inplace=True)\n",
    "train.fillna('', inplace=True)\n",
    "test = pd.read_csv(data_path / 'test.csv', header=None)\n",
    "test.fillna('', inplace=True)\n",
    "test.rename(columns={0: 'label', 1: 'question_title', 2: 'question_content', 3: 'best_answer'}, inplace=True)\n",
    "# Make the data into X and y\n",
    "X_train = train.drop('label', axis=1)\n",
    "y_train = train['label']\n",
    "X_test = test.drop('label', axis=1)\n",
    "y_test = test['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/theo-m1max/Documents/GitHub/YahooEmbeddings/test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/theo-m1max/Documents/GitHub/YahooEmbeddings/test.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m dummy_pipeline \u001b[39m=\u001b[39m Pipeline([(\u001b[39m'\u001b[39m\u001b[39mvectorizer\u001b[39m\u001b[39m'\u001b[39m, DistributedBagOfWords(lemmatize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, lowercase\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_stopwords\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_mean\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)), \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/theo-m1max/Documents/GitHub/YahooEmbeddings/test.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                 (\u001b[39m'\u001b[39m\u001b[39mmlp\u001b[39m\u001b[39m'\u001b[39m, DummyClassifier(strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstratified\u001b[39m\u001b[39m\"\u001b[39m))])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/theo-m1max/Documents/GitHub/YahooEmbeddings/test.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Fit the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/theo-m1max/Documents/GitHub/YahooEmbeddings/test.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m dummy_pipeline\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theo-m1max/Documents/GitHub/YahooEmbeddings/test.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m y_pred \u001b[39m=\u001b[39m dummy_pipeline\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theo-m1max/Documents/GitHub/YahooEmbeddings/test.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(classification_report(y_test, y_pred))\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/pipeline.py:378\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \n\u001b[1;32m    354\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> 378\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/pipeline.py:336\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    334\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[1;32m    335\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    337\u001b[0m     cloned_transformer,\n\u001b[1;32m    338\u001b[0m     X,\n\u001b[1;32m    339\u001b[0m     y,\n\u001b[1;32m    340\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    341\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    342\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[1;32m    343\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[1;32m    344\u001b[0m )\n\u001b[1;32m    345\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/pipeline.py:870\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    869\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 870\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    871\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/base.py:870\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    868\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n",
      "File \u001b[0;32m~/Documents/GitHub/YahooEmbeddings/models.py:70\u001b[0m, in \u001b[0;36mDistributedBagOfWords.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m     69\u001b[0m     \u001b[39m# Use numpy's apply_along_axis function to apply _transform2 to all rows of X\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mapply_along_axis(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform2, \u001b[39m1\u001b[39;49m, X)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/numpy/lib/shape_base.py:402\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m buff[ind0] \u001b[39m=\u001b[39m res\n\u001b[1;32m    401\u001b[0m \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m inds:\n\u001b[0;32m--> 402\u001b[0m     buff[ind] \u001b[39m=\u001b[39m asanyarray(func1d(inarr_view[ind], \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    404\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(res, matrix):\n\u001b[1;32m    405\u001b[0m     \u001b[39m# wrap the array, to preserve subclasses\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     buff \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39m__array_wrap__(buff)\n",
      "File \u001b[0;32m~/Documents/GitHub/YahooEmbeddings/models.py:63\u001b[0m, in \u001b[0;36mDistributedBagOfWords._transform2\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform2\u001b[39m(\u001b[39mself\u001b[39m, row):\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m     \u001b[39m# Concatenate the sentence vectors\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     sent1_vec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform1(row[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     64\u001b[0m     sent2_vec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform1(row[\u001b[39m1\u001b[39m])\n\u001b[1;32m     65\u001b[0m     sent3_vec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform1(row[\u001b[39m2\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/GitHub/YahooEmbeddings/models.py:44\u001b[0m, in \u001b[0;36mDistributedBagOfWords._transform1\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform1\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m     \u001b[39m# Sum the word vectors for each word in the sentence\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenize(sentence)\n\u001b[1;32m     46\u001b[0m     \u001b[39m# If the sentence is empty, return a vector of zeros\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(words) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/GitHub/YahooEmbeddings/models.py:19\u001b[0m, in \u001b[0;36mParagraphEmbedding._tokenize\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tokenize\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[1;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremove_stopwords:\n\u001b[0;32m---> 19\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_remove_stopwords(sentence)\n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m [token\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(sentence)]\n",
      "File \u001b[0;32m~/Documents/GitHub/YahooEmbeddings/models.py:29\u001b[0m, in \u001b[0;36mParagraphEmbedding._remove_stopwords\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_remove_stopwords\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[1;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlemmatize:\n\u001b[0;32m---> 29\u001b[0m         \u001b[39mreturn\u001b[39;00m [token\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(sentence) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop]\n\u001b[1;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;00m [token\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(sentence) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/spacy/language.py:1026\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1025\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1026\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1028\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/spacy/pipeline/transition_parser.pyx:253\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/spacy/pipeline/transition_parser.pyx:274\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/spacy/ml/tb_framework.py:33\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model, X, is_train):\n\u001b[0;32m---> 33\u001b[0m     step_model \u001b[39m=\u001b[39m ParserStepModel(\n\u001b[1;32m     34\u001b[0m         X,\n\u001b[1;32m     35\u001b[0m         model\u001b[39m.\u001b[39;49mlayers,\n\u001b[1;32m     36\u001b[0m         unseen_classes\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mattrs[\u001b[39m\"\u001b[39;49m\u001b[39munseen_classes\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     37\u001b[0m         train\u001b[39m=\u001b[39;49mis_train,\n\u001b[1;32m     38\u001b[0m         has_upper\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mattrs[\u001b[39m\"\u001b[39;49m\u001b[39mhas_upper\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m step_model, step_model\u001b[39m.\u001b[39mfinish_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/spacy/ml/parser_model.pyx:213\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/thinc/layers/concatenate.py:44\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m---> 44\u001b[0m     Ys, callbacks \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[layer(X, is_train\u001b[39m=\u001b[39mis_train) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers])\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Ys[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[1;32m     46\u001b[0m         data_l, backprop \u001b[39m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/thinc/layers/concatenate.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m---> 44\u001b[0m     Ys, callbacks \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[layer(X, is_train\u001b[39m=\u001b[39;49mis_train) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers])\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Ys[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[1;32m     46\u001b[0m         data_l, backprop \u001b[39m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/spacy/ml/staticvectors.py:43\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m _handle_empty(model\u001b[39m.\u001b[39mops, model\u001b[39m.\u001b[39mget_dim(\u001b[39m\"\u001b[39m\u001b[39mnO\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     42\u001b[0m key_attr: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mattrs[\u001b[39m\"\u001b[39m\u001b[39mkey_attr\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 43\u001b[0m keys \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mflatten([cast(Ints1d, doc\u001b[39m.\u001b[39mto_array(key_attr)) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs])\n\u001b[1;32m     44\u001b[0m vocab: Vocab \u001b[39m=\u001b[39m docs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mvocab\n\u001b[1;32m     45\u001b[0m W \u001b[39m=\u001b[39m cast(Floats2d, model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mas_contig(model\u001b[39m.\u001b[39mget_param(\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/spacy/ml/staticvectors.py:43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m _handle_empty(model\u001b[39m.\u001b[39mops, model\u001b[39m.\u001b[39mget_dim(\u001b[39m\"\u001b[39m\u001b[39mnO\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     42\u001b[0m key_attr: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mattrs[\u001b[39m\"\u001b[39m\u001b[39mkey_attr\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 43\u001b[0m keys \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mflatten([cast(Ints1d, doc\u001b[39m.\u001b[39;49mto_array(key_attr)) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs])\n\u001b[1;32m     44\u001b[0m vocab: Vocab \u001b[39m=\u001b[39m docs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mvocab\n\u001b[1;32m     45\u001b[0m W \u001b[39m=\u001b[39m cast(Floats2d, model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mas_contig(model\u001b[39m.\u001b[39mget_param(\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Setup Dummy Pipeline\n",
    "dummy_pipeline = Pipeline([('vectorizer', DistributedBagOfWords(lemmatize=True, lowercase=True, remove_stopwords=True, use_mean=False)), \n",
    "                ('mlp', DummyClassifier(strategy=\"stratified\"))])\n",
    "\n",
    "# Fit the model\n",
    "dummy_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dummy_pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.19      0.12      0.15        34\n",
      "           2       0.15      0.26      0.19        58\n",
      "           3       0.08      0.07      0.07        29\n",
      "           4       0.04      0.08      0.05        37\n",
      "           5       0.14      0.13      0.14        78\n",
      "           6       0.11      0.19      0.14        36\n",
      "           7       0.31      0.12      0.18       129\n",
      "           8       0.08      0.12      0.10        34\n",
      "           9       0.04      0.03      0.04        31\n",
      "          10       0.09      0.06      0.07        34\n",
      "\n",
      "    accuracy                           0.13       500\n",
      "   macro avg       0.12      0.12      0.11       500\n",
      "weighted avg       0.16      0.13      0.13       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup Dummy Pipeline\n",
    "dummy_pipeline = Pipeline([('vectorizer', DistributedBagOfWords(lemmatize=True, lowercase=True, remove_stopwords=True, use_mean=True)), \n",
    "                ('mlp', DummyClassifier(strategy=\"stratified\"))])\n",
    "\n",
    "# Fit the model\n",
    "dummy_pipeline.fit(X_train[0:500], y_train[0:500])\n",
    "\n",
    "y_pred = dummy_pipeline.predict(X_test[0:500])\n",
    "\n",
    "print(classification_report(y_test[0:500], y_pred[0:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.26      0.37        34\n",
      "           2       0.44      0.67      0.53        58\n",
      "           3       0.48      0.34      0.40        29\n",
      "           4       0.20      0.49      0.29        37\n",
      "           5       0.77      0.74      0.76        78\n",
      "           6       0.74      0.47      0.58        36\n",
      "           7       0.47      0.36      0.41       129\n",
      "           8       0.46      0.50      0.48        34\n",
      "           9       0.57      0.68      0.62        31\n",
      "          10       0.41      0.21      0.27        34\n",
      "\n",
      "    accuracy                           0.48       500\n",
      "   macro avg       0.51      0.47      0.47       500\n",
      "weighted avg       0.52      0.48      0.49       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Setup logistic regression pipeline\n",
    "logistic_regression_pipeline = Pipeline([('vectorizer', DistributedBagOfWords(lemmatize=True, lowercase=True, remove_stopwords=True, use_mean=False)),\n",
    "                ('mlp', LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000))])\n",
    "\n",
    "# Fit the model\n",
    "logistic_regression_pipeline.fit(X_train[0:500], y_train[0:500])\n",
    "\n",
    "y_pred = logistic_regression_pipeline.predict(X_test[0:500])\n",
    "\n",
    "print(classification_report(y_test[0:500], y_pred[0:500]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        34\n",
      "           2       0.34      0.81      0.48        58\n",
      "           3       0.00      0.00      0.00        29\n",
      "           4       0.17      0.51      0.25        37\n",
      "           5       0.70      0.82      0.76        78\n",
      "           6       0.67      0.50      0.57        36\n",
      "           7       0.54      0.41      0.46       129\n",
      "           8       0.67      0.53      0.59        34\n",
      "           9       1.00      0.10      0.18        31\n",
      "          10       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.44       500\n",
      "   macro avg       0.41      0.37      0.33       500\n",
      "weighted avg       0.45      0.44      0.40       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup logistic regression pipeline\n",
    "logistic_regression_pipeline = Pipeline([('vectorizer', DistributedBagOfWords(lemmatize=True, lowercase=True, remove_stopwords=True, use_mean=True)),\n",
    "                ('mlp', LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000))])\n",
    "\n",
    "# Fit the model\n",
    "logistic_regression_pipeline.fit(X_train[0:500], y_train[0:500])\n",
    "\n",
    "y_pred = logistic_regression_pipeline.predict(X_test[0:500])\n",
    "\n",
    "print(classification_report(y_test[0:500], y_pred[0:500], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.18      0.22        34\n",
      "           2       0.39      0.71      0.50        58\n",
      "           3       0.50      0.31      0.38        29\n",
      "           4       0.18      0.41      0.25        37\n",
      "           5       0.71      0.76      0.73        78\n",
      "           6       0.74      0.39      0.51        36\n",
      "           7       0.43      0.27      0.33       129\n",
      "           8       0.55      0.50      0.52        34\n",
      "           9       0.53      0.65      0.58        31\n",
      "          10       0.32      0.21      0.25        34\n",
      "\n",
      "    accuracy                           0.45       500\n",
      "   macro avg       0.46      0.44      0.43       500\n",
      "weighted avg       0.47      0.45      0.44       500\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        34\n",
      "           2       0.33      0.83      0.48        58\n",
      "           3       0.00      0.00      0.00        29\n",
      "           4       0.17      0.57      0.26        37\n",
      "           5       0.82      0.68      0.74        78\n",
      "           6       0.71      0.47      0.57        36\n",
      "           7       0.49      0.43      0.45       129\n",
      "           8       0.63      0.56      0.59        34\n",
      "           9       1.00      0.06      0.12        31\n",
      "          10       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.43       500\n",
      "   macro avg       0.41      0.36      0.32       500\n",
      "weighted avg       0.46      0.43      0.40       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# setup SVM pipeline\n",
    "svm_pipeline = Pipeline([('vectorizer', DistributedBagOfWords(lemmatize=True, lowercase=True, remove_stopwords=True, use_mean=False)),\n",
    "                ('mlp', SVC(kernel='linear'))])\n",
    "\n",
    "# Fit the model\n",
    "svm_pipeline.fit(X_train[0:500], y_train[0:500])\n",
    "\n",
    "y_pred = svm_pipeline.predict(X_test[0:500])\n",
    "\n",
    "print(classification_report(y_test[0:500], y_pred[0:500], zero_division=0))\n",
    "\n",
    "# setup svm pipeline\n",
    "svm_pipeline = Pipeline([('vectorizer', DistributedBagOfWords(lemmatize=True, lowercase=True, remove_stopwords=True, use_mean=True)),\n",
    "                ('mlp', SVC(kernel='linear'))])\n",
    "\n",
    "# Fit the model\n",
    "svm_pipeline.fit(X_train[0:500], y_train[0:500])\n",
    "\n",
    "y_pred = svm_pipeline.predict(X_test[0:500])\n",
    "\n",
    "print(classification_report(y_test[0:500], y_pred[0:500], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        34\n",
      "           2       0.37      0.45      0.41        58\n",
      "           3       1.00      0.03      0.07        29\n",
      "           4       0.09      0.78      0.16        37\n",
      "           5       0.80      0.41      0.54        78\n",
      "           6       0.71      0.14      0.23        36\n",
      "           7       0.49      0.16      0.24       129\n",
      "           8       1.00      0.06      0.11        34\n",
      "           9       0.33      0.06      0.11        31\n",
      "          10       0.50      0.03      0.06        34\n",
      "\n",
      "    accuracy                           0.24       500\n",
      "   macro avg       0.53      0.21      0.19       500\n",
      "weighted avg       0.53      0.24      0.24       500\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        34\n",
      "           2       0.29      0.83      0.43        58\n",
      "           3       0.88      0.24      0.38        29\n",
      "           4       0.24      0.54      0.33        37\n",
      "           5       0.74      0.76      0.75        78\n",
      "           6       0.76      0.44      0.56        36\n",
      "           7       0.53      0.43      0.47       129\n",
      "           8       0.66      0.56      0.60        34\n",
      "           9       0.89      0.26      0.40        31\n",
      "          10       0.67      0.06      0.11        34\n",
      "\n",
      "    accuracy                           0.47       500\n",
      "   macro avg       0.57      0.41      0.40       500\n",
      "weighted avg       0.56      0.47      0.45       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# setup SVM pipeline\n",
    "svm_pipeline = Pipeline([('vectorizer', DistributedBagOfWords(lemmatize=True, lowercase=True, remove_stopwords=True, use_mean=False)),\n",
    "                ('mlp', SVC(kernel='rbf', gamma='scale'))])\n",
    "\n",
    "# Fit the model\n",
    "svm_pipeline.fit(X_train[0:500], y_train[0:500])\n",
    "\n",
    "y_pred = svm_pipeline.predict(X_test[0:500])\n",
    "\n",
    "print(classification_report(y_test[0:500], y_pred[0:500], zero_division=0))\n",
    "\n",
    "# setup svm pipeline\n",
    "svm_pipeline = Pipeline([('vectorizer', DistributedBagOfWords(lemmatize=True, lowercase=True, remove_stopwords=True, use_mean=True)),\n",
    "                ('mlp', SVC(kernel='rbf', gamma='scale'))])\n",
    "\n",
    "# Fit the model\n",
    "svm_pipeline.fit(X_train[0:500], y_train[0:500])\n",
    "\n",
    "y_pred = svm_pipeline.predict(X_test[0:500])\n",
    "\n",
    "print(classification_report(y_test[0:500], y_pred[0:500], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.26      0.35        34\n",
      "           2       0.42      0.69      0.52        58\n",
      "           3       0.48      0.45      0.46        29\n",
      "           4       0.17      0.57      0.26        37\n",
      "           5       0.66      0.81      0.72        78\n",
      "           6       1.00      0.06      0.11        36\n",
      "           7       0.49      0.31      0.38       129\n",
      "           8       0.00      0.00      0.00        34\n",
      "           9       0.59      0.61      0.60        31\n",
      "          10       0.27      0.18      0.21        34\n",
      "\n",
      "    accuracy                           0.43       500\n",
      "   macro avg       0.46      0.39      0.36       500\n",
      "weighted avg       0.48      0.43      0.40       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# setup Gaussian Process pipeline\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "gaussian_process_pipeline = Pipeline([('vectorizer', DistributedBagOfWords(lemmatize=True, lowercase=True, remove_stopwords=True, use_mean=False)),\n",
    "                ('mlp', GaussianProcessClassifier(kernel))])\n",
    "\n",
    "# Fit the model\n",
    "gaussian_process_pipeline.fit(X_train[0:500], y_train[0:500])\n",
    "\n",
    "y_pred = gaussian_process_pipeline.predict(X_test[0:500])\n",
    "\n",
    "print(classification_report(y_test[0:500], y_pred[0:500], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        34\n",
      "           2       0.41      0.71      0.52        58\n",
      "           3       0.38      0.48      0.42        29\n",
      "           4       0.24      0.41      0.30        37\n",
      "           5       0.69      0.85      0.76        78\n",
      "           6       0.62      0.64      0.63        36\n",
      "           7       0.55      0.43      0.48       129\n",
      "           8       0.56      0.59      0.57        34\n",
      "           9       0.57      0.55      0.56        31\n",
      "          10       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.50       500\n",
      "   macro avg       0.40      0.47      0.42       500\n",
      "weighted avg       0.45      0.50      0.47       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup Gaussian Process pipeline\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "gaussian_process_pipeline = Pipeline([('vectorizer', DistributedBagOfWords(lemmatize=True, lowercase=True, remove_stopwords=True, use_mean=True)),\n",
    "                ('mlp', GaussianProcessClassifier(kernel))])\n",
    "\n",
    "# Fit the model\n",
    "gaussian_process_pipeline.fit(X_train[0:500], y_train[0:500])\n",
    "\n",
    "y_pred = gaussian_process_pipeline.predict(X_test[0:500])\n",
    "\n",
    "print(classification_report(y_test[0:500], y_pred[0:500], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.45      0.26      0.33        34\n",
      "           2       0.39      0.60      0.48        58\n",
      "           3       0.56      0.34      0.43        29\n",
      "           4       0.24      0.46      0.31        37\n",
      "           5       0.70      0.79      0.74        78\n",
      "           6       0.74      0.56      0.63        36\n",
      "           7       0.52      0.37      0.43       129\n",
      "           8       0.52      0.47      0.49        34\n",
      "           9       0.42      0.58      0.49        31\n",
      "          10       0.35      0.21      0.26        34\n",
      "\n",
      "    accuracy                           0.48       500\n",
      "   macro avg       0.49      0.47      0.46       500\n",
      "weighted avg       0.51      0.48      0.48       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# setup mlp pipeline\n",
    "mlp_pipeline = Pipeline([('vectorizer', DistributedBagOfWords(lemmatize=True, lowercase=True, remove_stopwords=True, use_mean=False)),\n",
    "                ('mlp', MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000, activation='relu', solver='adam', random_state=1337))])    \n",
    "\n",
    "# Fit the model\n",
    "mlp_pipeline.fit(X_train[0:500], y_train[0:500])\n",
    "\n",
    "y_pred = mlp_pipeline.predict(X_test[0:500])\n",
    "\n",
    "print(classification_report(y_test[0:500], y_pred[0:500], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.32      0.21      0.25        34\n",
      "           2       0.45      0.57      0.50        58\n",
      "           3       0.43      0.55      0.48        29\n",
      "           4       0.23      0.32      0.27        37\n",
      "           5       0.72      0.79      0.76        78\n",
      "           6       0.77      0.56      0.65        36\n",
      "           7       0.53      0.37      0.44       129\n",
      "           8       0.36      0.47      0.41        34\n",
      "           9       0.40      0.52      0.45        31\n",
      "          10       0.64      0.53      0.58        34\n",
      "\n",
      "    accuracy                           0.50       500\n",
      "   macro avg       0.49      0.49      0.48       500\n",
      "weighted avg       0.51      0.50      0.50       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup mlp pipeline\n",
    "mlp_pipeline = Pipeline([('vectorizer', DistributedBagOfWords(lemmatize=True, lowercase=True, remove_stopwords=True, use_mean=True)),\n",
    "                ('mlp', MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000, activation='relu', solver='adam', random_state=1337))])\n",
    "\n",
    "# Fit the model\n",
    "mlp_pipeline.fit(X_train[0:500], y_train[0:500])\n",
    "\n",
    "y_pred = mlp_pipeline.predict(X_test[0:500])\n",
    "\n",
    "print(classification_report(y_test[0:500], y_pred[0:500], zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:52:10) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce05b5a9bede28b862a1c4f446dcff6069cc570518307c655629599948881620"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
