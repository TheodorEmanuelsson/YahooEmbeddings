{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd notebooks/YahooEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from models import DistributedBagOfWords,MeanPooling,PCA_Projection\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0\n",
      "0       Society & Culture\n",
      "1   Science & Mathematics\n",
      "2                  Health\n",
      "3   Education & Reference\n",
      "4    Computers & Internet\n",
      "5                  Sports\n",
      "6      Business & Finance\n",
      "7   Entertainment & Music\n",
      "8  Family & Relationships\n",
      "9   Politics & Government\n"
     ]
    }
   ],
   "source": [
    "# Setup the paths\n",
    "cwd = Path.cwd()\n",
    "data_path = cwd / 'data'\n",
    "# Read the data\n",
    "classes = pd.read_csv(data_path / 'classes.txt', header = None)\n",
    "train = pd.read_csv(data_path / 'train.csv', header=None)\n",
    "train.rename(columns={0: 'label', 1: 'question_title', 2: 'question_content', 3: 'best_answer'}, inplace=True)\n",
    "train.fillna('', inplace=True)\n",
    "test = pd.read_csv(data_path / 'test.csv', header=None)\n",
    "test.fillna('', inplace=True)\n",
    "test.rename(columns={0: 'label', 1: 'question_title', 2: 'question_content', 3: 'best_answer'}, inplace=True)\n",
    "# Make the data into X and y\n",
    "X_train = train.drop('label', axis=1)\n",
    "y_train = train['label']\n",
    "X_test = test.drop('label', axis=1)\n",
    "y_test = test['label']\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       avg_title  avg_content  avg_answer\n",
      "label                                    \n",
      "1           10.0         10.0        37.0\n",
      "2            9.0          0.0        42.0\n",
      "3           10.0          9.0        43.0\n",
      "4            9.0          0.0        28.0\n",
      "5           11.0          8.0        29.0\n",
      "6           10.0          0.0        24.0\n",
      "7           10.0          0.0        29.0\n",
      "8            9.0          6.0        17.0\n",
      "9           10.0         18.0        34.0\n",
      "10          11.0          9.0        38.0\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the data to do a summary table of median words per class and data field\n",
    "X = pd.concat([X_train, X_test], axis=0)\n",
    "y = pd.concat([y_train, y_test], axis=0)\n",
    "data = pd.concat([X, y], axis=1)\n",
    "summary_data = data.assign(avg_title = data['question_title'].str.split().str.len(), \n",
    "                            avg_content = data['question_content'].str.split().str.len(),\n",
    "                            avg_answer = data['best_answer'].str.split().str.len()).groupby('label').agg({'avg_title': 'median', 'avg_content': 'median', 'avg_answer': 'median'})\n",
    "print(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the DBOW vectors\n",
      "Loading the MeanPool vectors\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Perform the preprocessing of the data once.\n",
    "# Set PROCESS to True if processing need to be runa\n",
    "PROCESS = False\n",
    "\n",
    "if PROCESS:\n",
    "    # Initialize the DBOW class and the MeanPool class that does preprocessing and vectorization\n",
    "    DBOW = DistributedBagOfWords(lemmatize=True, lowercase=True, remove_stopwords=True)\n",
    "    MeanPool = MeanPooling(lemmatize=True, lowercase=True, remove_stopwords=True)\n",
    "    PCA = PCA_Projection(lemmatize=True, lowercase=True, remove_stopwords=True)\n",
    "\n",
    "    # Run the DBOW on all the data and store it as numpy arrays\n",
    "    sum_vectors_train = DBOW.transform(X_train)\n",
    "    sum_vectors_test = DBOW.transform(X_test)\n",
    "    np.save('sum_vectors_train.npy', sum_vectors_train)\n",
    "    np.save('sum_vectors_test.npy', sum_vectors_test)\n",
    "\n",
    "    # Run the MeanPooling on all data and store it as numpy arrays\n",
    "    mean_vectors_train = MeanPool.transform(X_train)\n",
    "    mean_vectors_test = MeanPool.transform(X_test)\n",
    "    np.save('mean_vectors_train.npy', mean_vectors_train)\n",
    "    np.save('mean_vectors_test.npy', mean_vectors_test)\n",
    "\n",
    "    # Run the PCA_Projection on all the training data and store it as a numpy array\n",
    "    pca_vectors_train = PCA.transform(X_train)\n",
    "    pca_vectors_test = PCA.transform(X_test)\n",
    "    np.save('pca_vectors_train.npy', pca_vectors_train)\n",
    "    np.save('pca_vectors_test.npy', pca_vectors_test)\n",
    "else:\n",
    "    print('Loading the DBOW vectors')\n",
    "    sum_vectors_train = np.load('sum_vectors_train.npy')\n",
    "    sum_vectors_test = np.load('sum_vectors_test.npy')\n",
    "    print('Loading the MeanPool vectors')\n",
    "    mean_vectors_train = np.load('mean_vectors_train.npy')\n",
    "    mean_vectors_test = np.load('mean_vectors_test.npy')\n",
    "    print('Loading the PCA projected vectors')\n",
    "    pca_vectors_train = np.load('pca_vectors_train.npy')\n",
    "    pca_vectors_test = np.load('pca_vectors_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 ms, sys: 1.52 ms, total: 18.1 ms\n",
      "Wall time: 28 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the logistic regression models\n",
    "lr_DBOW = LogisticRegression(max_iter=200, random_state=1337)\n",
    "lr_MeanPool = LogisticRegression(max_iter=200, random_state=1337)\n",
    "lr_PCA = LogisticRegression(max_iter=200, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 9min 59s, sys: 18min 27s, total: 1h 28min 26s\n",
      "Wall time: 4min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=1337)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=1337)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=1337)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Fit the DBOW model\n",
    "lr_DBOW.fit(sum_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the MeanPool model\n",
    "lr_MeanPool.fit(mean_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the PCA model\n",
    "lr_PCA.fit(pca_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize linear SVM models\n",
    "svml_DBOW = SVC(kernel='linear', random_state=1337)\n",
    "svml_MeanPool = SVC(kernel='linear', random_state=1337)\n",
    "svml_PCA = SVC(kernel='linear', random_state=1337)\n",
    "\n",
    "# Initialize radial basis function SVM\n",
    "svmrbf_DBOW = SVC(kernel='rbf', gamma='scale', random_state=1337)\n",
    "svmrbf_MeanPool = SVC(kernel='rbf', gamma='scale', random_state=1337)\n",
    "svmrbf_PCA = SVC(kernel='rbf', gamma='scale', random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the DBOW SVM model with linear kernel\n",
    "svml_DBOW.fit(sum_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the DBOW SVM model with linear RBF kernel\n",
    "svmrbf_DBOW.fit(sum_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the MeanPool SVM model with linear kernel\n",
    "svml_MeanPool.fit(mean_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the MeanPool SVM model with RBF kernel\n",
    "svmrbf_MeanPool.fit(mean_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the PCA SVM model with linear kernel\n",
    "svml_PCA.fit(pca_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "svmrbf_PCA.fit(pca_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initialize the MLP models\n",
    "mlp_DBOW = MLPClassifier(hidden_layer_sizes=(100, 100, 100), \n",
    "                         max_iter=200, activation='relu', \n",
    "                         solver='adam', random_state=1337)\n",
    "mlp_MeanPool = MLPClassifier(hidden_layer_sizes=(100, 100, 100), \n",
    "                             max_iter=200, activation='relu', \n",
    "                             solver='adam', random_state=1337)\n",
    "mlp_PCA = MLPClassifier(hidden_layer_sizes=(100, 100, 100), \n",
    "                        max_iter=200, activation='relu', \n",
    "                        solver='adam', random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the DBOW MLP model\n",
    "mlp_DBOW.fit(sum_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the MeanPool MLP model\n",
    "mlp_MeanPool.fit(mean_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Fit the PCA MLP model\n",
    "mlp_PCA.fit(pca_vectors, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce05b5a9bede28b862a1c4f446dcff6069cc570518307c655629599948881620"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
